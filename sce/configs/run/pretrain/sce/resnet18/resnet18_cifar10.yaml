# copyright CEA-LIST/DIASI/SIALV/LVA (2022)
# author CEA-LIST/DIASI/SIALV/LVA <julien.denize@cea.fr>
# license CeCILL version 2.1

defaults:
  - /callback/model_checkpoint@callbacks.model_checkpoint: epoch
  - /callback/online_evaluator@callbacks.online_evaluator: adam_online_evaluator
  - /model/head/linear@callbacks.online_evaluator.classifier: in512_out10
  - /callback/learning_rate_monitor@callbacks.learning_rate_monitor: default
  - /callback/progress_bar@callbacks.progress_bar: default
  - /datamodule: cifar10
  - /transform/multi_crop_transform@datamodule.train.transform: two_crops_soft_and_strong_pil_contrastive_cifar10
  - /model/siamese/sce@model: default
  - /model/head/mlp@model.projector: projector_resnet18_mlp2_bn1
  - /model/trunk/resnet/resnet18@model.trunk: small_input_no_fc
  - /optimizer/factory/sgd@model.optimizer: default
  - /scheduler/factory/linear_warmup_cosine_annealing_lr@model.optimizer.scheduler: default
  - /seed/seed_everything@seed: default
  - /trainer: gpu_ddp_amp
  - _self_
  
callbacks:
  model_checkpoint:
    dirpath: pretrain_checkpoints
    every_n_epochs: 100
    filename: '{epoch}'
    save_last: False
    save_top_k: -1
  online_evaluator:
    input_name: h
    optimizer:
      batch_size: ${....datamodule.train.global_batch_size}
    classifier:
      output_dim: 10
    precision: ${...trainer.precision}
  progress_bar:
    refresh_rate: 10
ckpt_path: null
datamodule:
  datadir: ${..dir.data}
  train:
    loader:
      drop_last: True
      num_workers: 4
      pin_memory: True
    global_batch_size: 256
model:
  queue:
    size: 4096
    feature_dim: ${..projector.output_dim}
  optimizer:
    batch_size: ${...datamodule.train.global_batch_size}
    exclude_wd_norm: False
    exclude_wd_bias: False
    params:
      nesterov: False
      weight_decay: 0.0005
    initial_lr: 0.06
    scaler: linear
    scheduler:
      params:
        max_epochs: ${.....trainer.max_epochs}
        warmup_epochs: 5
  initial_momentum: 0.99
  scheduler_momentum: constant
  num_devices: ${..trainer.gpus}
  shuffle_bn: True
  simulate_n_devices: 8
  temp: 0.1
  temp_m: 0.05
  initial_temp_m: 0.05
  warmup_epoch_temp_m: 0
  coeff: 0.5
seed:
  seed: 42
trainer:
  max_epochs: 200
  gpus: 1
dir:
  data: data/
  root: outputs/sce
  exp: pretrain_batch_size${..datamodule.train.global_batch_size}_epoch${..trainer.max_epochs}_temp${..model.temp}_seed${..seed.seed}/pretrain
  run: ${.root}/${.exp}

hydra:
  searchpath: 
    - pkg://sce.configs
  run:
    dir: ${...dir.run}
